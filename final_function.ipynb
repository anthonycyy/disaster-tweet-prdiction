{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import contractions\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the stopwords, punctuation and lemmatizer\n",
    "punc=string.punctuation\n",
    "stpwrd = stopwords.words('english')\n",
    "stpwrd.extend(['...','wa','ha','via',\"'s\",'amp'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "l_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('train_dt.csv')\n",
    "test=pd.read_csv('test_dt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning and feature engineering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the vectorizers,scaler and models\n",
    "clf = load('clf.joblib') \n",
    "hashtags_vec=load('hashtags_vec.joblib')\n",
    "text_vec=load('text_vec.joblib')\n",
    "scaler=load('scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a function to find all hashtags in text\n",
    "def find_hashtag(text):\n",
    "    hashtags=re.findall(r'#(\\w+)',text.lower())\n",
    "    hashtags=\" \".join(hashtags)\n",
    "    return hashtags\n",
    "\n",
    "#a function to clean the text\n",
    "def clean_text_(text,stemmer):\n",
    "    #remove regex\n",
    "    regexs = [\"(http|www)(\\S+)\", #remove URL\n",
    "              r'(@.+?)\\s', #remove twitter usernames\n",
    "                  r'\\d+', #remove numbers\n",
    "                  r'(//t.co/.+?)\\s', #remove twitter shortened URLs\n",
    "              r'#(\\w+)', #remove hashtags\n",
    "              r'[^\\x00-\\x7F]+' #remove non_english characters\n",
    "             ]\n",
    "    for regex in regexs:\n",
    "        text=re.sub(regex,\"\",text.lower())\n",
    "    #expand the contractions\n",
    "    words=text.lower().split()\n",
    "    words=[contractions.fix(word) for word in words]\n",
    "    text=\" \".join(words)\n",
    "    #tokenize text\n",
    "    words=word_tokenize(text.lower())\n",
    "    #stem or lamatize the tokens\n",
    "    words=[stemmer(word) for word in words]\n",
    "    #remove stopwords and punctuation\n",
    "    words=[word for word in words if word not in stpwrd]\n",
    "    words=[word for word in words if word not in punc]\n",
    "    text=\" \".join(words)\n",
    "    return text\n",
    "\n",
    "#a function to apply word countvectorizer\n",
    "def apply_vectorizer(df,col,vectorizer):\n",
    "    features = vectorizer.transform(df[col])\n",
    "    features = features.toarray()\n",
    "    df_features=pd.DataFrame(data=features,columns=vectorizer.get_feature_names_out(),index=df.index)\n",
    "    return df_features\n",
    "\n",
    "#a function combined with data cleaning and feature engineering\n",
    "def clean_eng(train):\n",
    "    \n",
    "    train=train[['text']].copy()\n",
    "    \n",
    "    #find length of text\n",
    "    train['text_length']=train['text'].map(len)\n",
    "\n",
    "    #find number of upper case characters in text\n",
    "    train['nos_uppercase']=train['text'].apply(lambda text:sum(char.isupper() for char in text))\n",
    "\n",
    "    #find number of punctuation in text\n",
    "    train['nos_punc']=train['text'].apply(lambda text:sum(char in punc for char in text))\n",
    "\n",
    "\n",
    "    #find number of words not include punctuation \n",
    "    train['nos_words']=train['text'].\\\n",
    "    apply(lambda text:len([word for word in word_tokenize(text) if word not in punc]))\n",
    "\n",
    "    #find percentage of upper case characters in text\n",
    "    train['perc_uppercase']=train['nos_uppercase']/train['text_length']\n",
    "    #find percentage of punctuation in text\n",
    "    train['perc_punc']=train['nos_punc']/train['text_length']\n",
    "\n",
    "\n",
    "    #find all hashtags in text\n",
    "    train['hashtags']=train['text'].apply(find_hashtag)\n",
    "\n",
    "    #find number of hashtags in text\n",
    "    train['nos_hashtags']=train['hashtags'].apply(lambda text:len(text.split()))\n",
    "    #find percentage of hashtags in text\n",
    "    train['perc_hashtags']=train['nos_hashtags']/train['nos_words']\n",
    "\n",
    "    #apply cleaning text function\n",
    "    train['clean_text']=train['text'].apply(clean_text_,stemmer=lemmatizer.lemmatize)\n",
    "    \n",
    "    all_cols=list(train.columns)\n",
    "\n",
    "    numeric_cols=[]\n",
    "    text_cols=[]\n",
    "    for col in all_cols:\n",
    "        data_type=train[col].dtype\n",
    "        if data_type=='O':\n",
    "            text_cols.append(col)\n",
    "        else:\n",
    "            numeric_cols.append(col)\n",
    "\n",
    "    #remove original text        \n",
    "    if 'text' in text_cols:\n",
    "        text_cols.remove('text')\n",
    "    \n",
    "    #remove number of uppercase, punctuation and hashtags columns\n",
    "    #they are used to create the respective percentage columns\n",
    "    for col in ['nos_uppercase','nos_punc','nos_hashtags']:\n",
    "        numeric_cols.remove(col)\n",
    "    \n",
    "    train=train[numeric_cols+text_cols]     \n",
    "    \n",
    "    #apply the word countvectorizer\n",
    "    train_text=apply_vectorizer(train,'clean_text',text_vec)\n",
    "    train_hashtags=apply_vectorizer(train,'hashtags',hashtags_vec)\n",
    "    #join the data of numeric columns with the data of cleaned text and data of hashtags after apply vectorizer\n",
    "    train_join=train[numeric_cols]\\\n",
    "        .join(train_text)\\\n",
    "        .join(train_hashtags,lsuffix='t',rsuffix='h')\n",
    "    \n",
    "    #apply scaling to numeric columns that is not percentage\n",
    "    scaler_cols=[col for col in numeric_cols if not re.match(r\"perc\\w*\",col)]\n",
    "    \n",
    "    #apply the min max scaler\n",
    "    scaled_features=scaler.transform(train[scaler_cols])\n",
    "    scaled_features=pd.DataFrame(data=scaled_features,columns=scaler.feature_names_in_,index=train.index)\n",
    "    for col in scaled_features:\n",
    "        train_join[col]=scaled_features[col]\n",
    "    \n",
    "    return train_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6457296231375262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my_tweet=\"Tornado is coming. We need help. Please help us. #SOS #disaster #Tornado #emergency\"\n",
    "# df_my_tweet=pd.DataFrame(data=[my_tweet],columns=['text'])\n",
    "# df_my_tweet=clean_eng(df_my_tweet)\n",
    "# clf.predict_proba(df_my_tweet)[:,1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final function for prediction <br>\n",
    "input: tweet: str, cut off (default to be 0.5): float <br>\n",
    "output: probablility of the tweet is about real disaster: float, True if probability > cut off False otherwise <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_disaster_tweet(tweet,cut_off=0.5):\n",
    "    df_tweet=clean_eng(pd.DataFrame(data=[tweet],columns=['text']))\n",
    "    proba=clf.predict_proba(df_tweet)[:,1][0]\n",
    "    return proba,proba>cut_off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the followings are the demo of function based on tweets written by myself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7782116747187081, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=\"Emergency situation. There is a wildfire. PLEASE SEND HELP. If no help, people will die. #SOS #WILDFIRE\"\n",
    "predict_disaster_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5482150664041652, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=\"SOS! Please send help. Emergency situation. People is suffering. #SOS #Emergency\"\n",
    "predict_disaster_tweet(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non disaster tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13317355414206983, False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet='Haha! Send help ar! Someone needs help ar!'\n",
    "predict_disaster_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17018757993706637, False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=\"Cry! He is dying! Call 911 la!\"\n",
    "predict_disaster_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS\n",
      "(0.08814375797465712, False)\n",
      "\n",
      "Help us. Hurricane is happening. Many people are hurt. #emergency #911 #hurricane\n",
      "(0.5717238491318939, True)\n",
      "\n",
      "God save the queen\n",
      "(0.0750019085660341, False)\n",
      "\n",
      "Wildfire is happening. Send help. The fire is growing strongly. If no help, people die.\n",
      "(0.7194949112207033, True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets=[\n",
    "    \"SOS\",\n",
    "    \"Help us. Hurricane is happening. Many people are hurt. #emergency #911 #hurricane\",\n",
    "    \"God save the queen\",\n",
    "    \"Wildfire is happening. Send help. The fire is growing strongly. If no help, people die.\",  \n",
    "]\n",
    "for tweet in tweets:\n",
    "    print(tweet)\n",
    "    print(predict_disaster_tweet(tweet))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demo in my capsetone presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omg, I was on the street and I saw a giant dog barking, I was so scared #doglife\n",
      "(0.3231025403944372, False)\n",
      "\n",
      "No milk in the shopps? deffo sign of the apocolipse! #endisneigh\n",
      "(0.18842359005460785, False)\n",
      "\n",
      "my shoes are full of salt! Please send help! this is worse than chernobyl! #FootDisaster #Sore\n",
      "(0.15275135156274358, False)\n",
      "\n",
      "OMG I just felt the worst earthquake ever, what is happening? #wtf\n",
      "(0.46850133798744703, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets=[\n",
    "    \"Omg, I was on the street and I saw a giant dog barking, I was so scared #doglife\",\n",
    "    \"No milk in the shopps? deffo sign of the apocolipse! #endisneigh\",\n",
    "    \"my shoes are full of salt! Please send help! this is worse than chernobyl! #FootDisaster #Sore\",\n",
    "    \"OMG I just felt the worst earthquake ever, what is happening? #wtf\"\n",
    "]\n",
    "for tweet in tweets:\n",
    "    print(tweet)\n",
    "    print(predict_disaster_tweet(tweet))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danger, Danger. High voltage. When we touch and when we kiss\n",
      "(0.13220105698386633, False)\n",
      "\n",
      "my goldfish is on fire\n",
      "(0.6385670502702395, True)\n",
      "\n",
      "Im at Kaggle and everyone is panicing! some random trainee data scientist just came in and smashed our competition in less then a week #catastrophy\n",
      "(0.3182729277507906, False)\n",
      "\n",
      "Theres a Random Forest Fire! #TheBetterModel\n",
      "(0.8384814278167484, True)\n",
      "\n",
      "I just heard the ISS was smashed by some random bar and pieces are crashing back to earth #doomed \n",
      "(0.19630275230755156, False)\n",
      "\n",
      "Oh no, pineapples are falling from the sky! #Apocalyse \n",
      "(0.2564231449932489, False)\n",
      "\n",
      "Incredibly sad to hear about the ongoing conflict on Jupiter resulting in the death of 16 McNuggets. My heart goes out to the delicious victims #StopWar\n",
      "(0.6432899159438875, True)\n",
      "\n",
      "Ran out of coffee #endoftheworld \n",
      "(0.18701731973500244, False)\n",
      "\n",
      "Danger, Danger. High voltage. When we touch and when we kiss\n",
      "(0.13220105698386633, False)\n",
      "\n",
      "my goldfish is on fire\n",
      "(0.6385670502702395, True)\n",
      "\n",
      "Im at Kaggle and everyone is panicing! some random trainee data scientist just came in and smashed our competition in less then a week #catastrophy\n",
      "(0.3182729277507906, False)\n",
      "\n",
      "Theres a Random Forest Fire! #TheBetterModel\n",
      "(0.8384814278167484, True)\n",
      "\n",
      "I just heard the ISS was smashed by some random bar and pieces are crashing back to earth #doomed \n",
      "(0.19630275230755156, False)\n",
      "\n",
      "Oh no, pineapples are falling from the sky! #Apocalyse \n",
      "(0.2564231449932489, False)\n",
      "\n",
      "Incredibly sad to hear about the ongoing conflict on Jupiter resulting in the death of 16 McNuggets. My heart goes out to the delicious victims #StopWar\n",
      "(0.6432899159438875, True)\n",
      "\n",
      "Ran out of coffee #endoftheworld \n",
      "(0.18701731973500244, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = [\n",
    "    \"Danger, Danger. High voltage. When we touch and when we kiss\",\n",
    "    \"my goldfish is on fire\",\n",
    "    \"Im at Kaggle and everyone is panicing! some random trainee data scientist just came in and smashed our competition in less then a week #catastrophy\",\n",
    "    \"Theres a Random Forest Fire! #TheBetterModel\",\n",
    "    \"I just heard the ISS was smashed by some random bar and pieces are crashing back to earth #doomed \",\n",
    "    \"Oh no, pineapples are falling from the sky! #Apocalyse \",\n",
    "    \"Incredibly sad to hear about the ongoing conflict on Jupiter resulting in the death of 16 McNuggets. My heart goes out to the delicious victims #StopWar\",\n",
    "    \"Ran out of coffee #endoftheworld \"\n",
    "\n",
    "]\n",
    "for tweet in tweets:\n",
    "    print(tweet)\n",
    "    print(predict_disaster_tweet(tweet))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model for competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply my model to data (test.csv) in Kaggle competition. I created a submission file for competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cleaned=clean_eng(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>nos_words</th>\n",
       "      <th>perc_uppercase</th>\n",
       "      <th>perc_punc</th>\n",
       "      <th>perc_hashtags</th>\n",
       "      <th>aa</th>\n",
       "      <th>aba</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abbswinston</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>wx</th>\n",
       "      <th>yazidis</th>\n",
       "      <th>yesh</th>\n",
       "      <th>yonews</th>\n",
       "      <th>youtubeh</th>\n",
       "      <th>yugvani</th>\n",
       "      <th>yyc</th>\n",
       "      <th>yycstorm</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zionist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.161290</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.612903</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3005 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_length  nos_words  perc_uppercase  perc_punc  perc_hashtags   aa  aba  \\\n",
       "0     0.180000   0.161290        0.029412   0.000000       0.000000  0.0  0.0   \n",
       "1     0.380000   0.258065        0.015625   0.046875       0.111111  0.0  0.0   \n",
       "2     0.593333   0.612903        0.010417   0.020833       0.000000  0.0  0.0   \n",
       "3     0.220000   0.096774        0.050000   0.075000       0.500000  0.0  0.0   \n",
       "4     0.253333   0.225806        0.088889   0.000000       0.000000  0.0  0.0   \n",
       "\n",
       "   abandoned  abbswinston  abc  ...   wx  yazidis  yesh  yonews  youtubeh  \\\n",
       "0        0.0          0.0  0.0  ...  0.0      0.0   0.0     0.0       0.0   \n",
       "1        0.0          0.0  0.0  ...  0.0      0.0   0.0     0.0       0.0   \n",
       "2        0.0          0.0  0.0  ...  0.0      0.0   0.0     0.0       0.0   \n",
       "3        0.0          0.0  0.0  ...  0.0      0.0   0.0     0.0       0.0   \n",
       "4        0.0          0.0  0.0  ...  0.0      0.0   0.0     0.0       0.0   \n",
       "\n",
       "   yugvani  yyc  yycstorm  zionism  zionist  \n",
       "0      0.0  0.0       0.0      0.0      0.0  \n",
       "1      0.0  0.0       0.0      0.0      0.0  \n",
       "2      0.0  0.0       0.0      0.0      0.0  \n",
       "3      0.0  0.0       0.0      0.0      0.0  \n",
       "4      0.0  0.0       0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 3005 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=clf.predict(test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission=pd.DataFrame()\n",
    "df_submission['id']=test['id']\n",
    "df_submission['target']=y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
